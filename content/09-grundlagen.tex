%! Author = mariuszindel
%! Date = 25.01.21

\section{Grundlagen}




\subsection{Entscheidungsfluss}
Der Entscheidungsfluss ist definiert als\\
\colorbox{lightlightgrey}{$H_{0}^{*}=\frac{\log _{2}(N)}{\tau}\left[\frac{b i t}{s}\right]$} \\
wobei $\tau$ die Zeit ist, die zur Übertragung eines Quellzeichens benötigt wird.


\subsection{Informationsgehalt $I(x_k)$}
Der Informationsgehalt eines Zeichens sagt aus, wie viele Elementarentscheidungen zur Bestimmung dieses Zeichens zu treffen sind.
\colorbox{lightlightgrey}{$\textcolor{blue}{I(x_k)} = -log_2(P(x_k))$}


\subsection{Entropie H(X)}
Der Informationsgehalt eines Zeichens sagt aus, wie viele Die Entropie bezeichnet den mittleren Informationsgehalt der Quelle. Sie Elementarentscheidungen zur Bestimmung dieses Zeichens zu treffen zeigt also auf, wie viele Elementarentscheidungen die Quelle/Senke im Mittel sind.\\
\colorbox{lightlightgrey}{$\textcolor{blue}{H(X)} = \sum_{k = 1}^{N}p(x_k) * I(x_k)$} [bit/Zeichen]\\
Die Entropie wird maximal, wenn jedes Zeichen gleichwahrscheinlich ist!





















\subsection{Codierungstheorem von Shannon}
\colorbox{lightlightgrey}{$H(x)<L<H(x) + 1$}

\subsection{Quellen ohne Gedächnis}
Allgemein kann nicht von einer gedächtnislosen Quelle ausgegangen werden!




\subsection{Quellen mit Gedächnis (Markov-Quelle)}
\subsection{Bedingte Wahrscheinlichkeit $P(Y|X)$}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
    \hline
    P(Y$|$X) & y1 & y2 & y3\\
    \hline
    x1 & $(x_1,y_1)$ & $(x_1,y_2)$ & $(x_1,y_3)$\\
    \hline
    x2 & $(x_2,y_1)$ & $(x_2,y_2)$ & $(x_2,y_3)$\\
    \hline
    x3 & $(x_3,y_1)$ & $(x_3,y_2)$ & $(x_3,y_3)$ \\
    \hline
\end{tabular}
\end{center}

\colorbox{lightlightgrey}{P(x1) = $P(x1) \times (x_1,y_1) + P(x2) \times (x_2,y_1) $}\\\colorbox{lightlightgrey}{$ + P(x3) \times (x_3,y_1)$}\\\\
\colorbox{lightlightgrey}{P(x2) = $P(x1) \times (x_1,y_2) + P(x2) \times (x_2,y_2) $}\\\colorbox{lightlightgrey}{$ + P(x3) \times (x_2,y_3)$}\\\\
\colorbox{lightlightgrey}{P(x3) = $P(x1) \times (x_1,y_3) + P(x2) \times (x_3,y_2) $}\\\colorbox{lightlightgrey}{$ + P(x3) \times (x_3,y_3)$}\\\\
\colorbox{lightlightgrey}{1 = P(x1) + P(x2)+ P(x3)}\\
Das ganze ist ein Gleichungssystem und muss nach P(x1), P(x2) und P(x3) aufgelöst werden!


\subsection{Verbundwahrscheinlichkeit $P(X,Y)$}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
    \hline
    P(X$,$Y) & y1 & y2 & y3\\
    \hline
    x1 & $(x_1,y_1)$ & $(x_1,y_2)$ & $(x_1,y_3)$\\
    \hline
    x2 & $(x_2,y_1)$ & $(x_2,y_2)$ & $(x_2,y_3)$\\
    \hline
    x3 & $(x_3,y_1)$ & $(x_3,y_2)$ & $(x_3,y_3)$ \\
    \hline
\end{tabular}
\end{center}\\

\colorbox{lightlightgrey}{$P(X,Y) = P(X_i) * P(Y_k | X_i)$}\\
$P(Y_k | X_i) = $ Zellen aus Tabelle der bedingten Wahrscheinlichkeit


\subsubsection{Verbundentropie $H(X,Y)$}
Wird so berechnet, dass für jede Zeile in der Tabelle von P(X,Y), die Entropie(\colorbox{lightlightgrey}{$P(x_i,y_i)*-log_2(P(x_i, y_i))$}) berechnet und am schluss alles Addiert wird(Beachte obwohl addiert wird hat es immer ein $-$MINUS! dazwischen!).\\
\colorbox{lightlightgrey}{$-P(x1,y1) * log_2(P(x1,y1)) - P(x2,y2) * log_2(P(x2,y2)$ usw.}
\subsubsection{Bedingte Entropie $H(Y|X)$}
\colorbox{lightlightgrey}{$H(Y|X) = H(X,Y) - H(X)$}











\vfill
$$
